{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp3XPuaTu9jl"
      },
      "source": [
        "\n",
        "# How to generate text: using different decoding methods for language generation with Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GPT-2**\n",
        "\n",
        "#### ***Language Modeling***\n",
        "- ê¸°ì¡´ GPT-1ì€ pre-trainingê³¼ supervised fine-tuningì˜ ê²°í•©\n",
        "- GPT-2ëŠ” language modeling(token sequenceë¥¼ ì‚¬ìš©í•´ ë¬¸ì¥ì˜ ë¹„ì§€ë„ ë¶„í¬ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•)\n",
        "\n",
        "#### ***Various Training Dataset***\n",
        "- GPT-2ì˜ ê°€ì¥ í° ëª©ì ì€ Fine-tuning ì—†ì´ unsupervised pre-training ë§Œì„ Zero-shotìœ¼ë¡œ í†µí•´ ë‹¤ì–‘í•œ taskë¥¼ ì§„í–‰í•  ìˆ˜ ìˆëŠ” General Language Modelì„ ë§Œë“œëŠ” ê²ƒ\n",
        "- GPT-1ì€ news article, wikipediaë¥¼ ì£¼ë¡œ ì‚¬ìš©. GPT-2ëŠ” Web Text ì‚¬ìš©\n",
        "\n",
        "#### ***Byte Pair Encoding(BPE)***\n",
        "- subword ê¸°ë°˜ì˜ ì¸ì½”ë”© ë°©ë²•ìœ¼ë¡œ ë¬¸ì ë‹¨ìœ„ë¡œ ë‹¨ì–´ë¥¼ ë¶„í•´í•´ vocab ìƒì„±\n",
        "- OOV(Out Of Vocabulary) ë¬¸ì œ í•´ê²° "
      ],
      "metadata": {
        "id": "19S3UvyanLnB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fK2bQ_rS02Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxLvv6UaPa33"
      },
      "source": [
        "### **Introduction**\n",
        "\n",
        "ìµœê·¼, ëŒ€ëŸ‰ì˜ weppage ë°ì´í„°ë¡œ í•™ìŠµ ëœ transformer-based language modelsì˜ ë“±ì¥ìœ¼ë¡œ open-ended language generationì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì¡ŒìŠµë‹ˆë‹¤.(ex. GPT-2) \n",
        "\n",
        "í–¥ìƒëœ tranformer architecureì™€ ëŒ€ëŸ‰ì˜ unsupervised trainig dataì™¸ì—ë„ ë” ë‚˜ì€ decoding methods ë˜í•œ ì¤‘ìš”í•œ ì—­í• ì„ í–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "ë³¸ ìë£Œì—ì„œëŠ” ë‹¤ì–‘í•œ decoding strategiesì— ëŒ€í•œ ê°„ëµí•œ ê°œìš”ì™€  `transformers` libraryë¥¼ ì‚¬ìš©í•´ ì†ì‰½ê²Œ ì´ëŸ¬í•œ decoding strategiesë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "ëª¨ë“  ë°©ë²•ë“¤ì€ **auto-regressive** language generation([here](http://jalammar.github.io/illustrated-gpt2/) a refresher)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![GPT-2 Auto regressive](http://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif)"
      ],
      "metadata": {
        "id": "0Zbm10Z96Cwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Auto-regressive* language generationëŠ” word sequenceì˜ í™•ë¥ ì€ next wordì˜ conditional distributionìœ¼ë¡œ decomposeí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ê¸°ë³¸ ê°€ì •ì…ë‹ˆë‹¤. \n",
        "$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\text{ ,with }  w_{1: 0} = \\emptyset, $$\n",
        "\n",
        "$W_0$ëŠ” initial *context* word sequenceë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. Word sequenceì˜ ê¸¸ì´ì¸ $T$ëŠ” $P(w_{t} | w_{1: t-1}, W_{0})$ìœ¼ë¡œ ë¶€í„° EOS tokenì´ ë‚˜ì˜¨ timestep $t=T$ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "Auto-regressive language generationì€ `GPT2`, `XLNet`, `OpenAi-GPT`, `CTRL`, `TransfoXL`, `XLM`, `Bart`, `T5` in both PyTorch and Tensorflow >= 2.0!ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì´ë²ˆ ì‹¤ìŠµ ì‹œê°„ì—ëŠ” í˜„ì¬ ìœ ëª…í•œ decoding ë°©ë²•ë“¤ì¸ *Greedy search*, *Beam search*, *Top-K sampling*, *Top-p sampling* ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ToPf40Ab6J8j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si4GyYhOQMzi"
      },
      "source": [
        "Transformerë¥¼ ì„¤ì¹˜í•˜ê³  Modelì„ load í•´ë´…ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbzZ_IVTtoQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7f53f1-7cda-44d2-bb92-e3513755c2a8"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ë²ˆ ì‹¤ìŠµ ì‹œê°„ì—ì„œëŠ” SKTì—ì„œ ê³µê°œí•œ KoGPT-2 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "95OFOKD_-z8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!apt-get install git-lfs\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/taeminlee/kogpt2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L6NtIzO9awd",
        "outputId": "b6bfd360-39e2-40c5-ee63-940500e27ea6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected operating system as Ubuntu/focal.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Detected apt version as 2.0.9\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n",
            "done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages will be upgraded:\n",
            "  git-lfs\n",
            "1 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 7,419 kB of archives.\n",
            "After this operation, 4,936 kB of additional disk space will be used.\n",
            "Get:1 https://packagecloud.io/github/git-lfs/ubuntu focal/main amd64 git-lfs amd64 3.3.0 [7,419 kB]\n",
            "Fetched 7,419 kB in 1s (7,858 kB/s)\n",
            "(Reading database ... 122353 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_3.3.0_amd64.deb ...\n",
            "Unpacking git-lfs (3.3.0) over (2.9.2-1) ...\n",
            "Setting up git-lfs (3.3.0) ...\n",
            "Git LFS initialized.\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Git LFS initialized.\n",
            "Cloning into 'kogpt2'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Total 52 (delta 0), reused 0 (delta 0), pack-reused 52\u001b[K\n",
            "Unpacking objects: 100% (52/52), 1.52 MiB | 1.52 MiB/s, done.\n",
            "Filtering content: 100% (2/2), 959.93 MiB | 23.98 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue2kOQhXTAMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50678257-1090-4021-bb61-89468c26f6c4"
      },
      "source": [
        "import torch\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = SentencePieceBPETokenizer(\"/content/kogpt2/vocab.json\", \"/content/kogpt2/merges.txt\")\n",
        "\n",
        "# ì´ ëª¨ë¸ê°™ì€ ê²½ìš° vocab_sizeê°€ 50000ë¡œ ë˜ì–´ìˆìŒ.\n",
        "# ê·¸ë˜ì„œ config GPT2 loadë¥¼ í•  ë•Œ ë°˜ë“œì‹œ vocab sizeë¥¼ ë§ì¶°ì¤˜ì•¼ í•¨.\n",
        "config = GPT2Config(vocab_size=50000)\n",
        "config.pad_token_id = tokenizer.token_to_id('<pad>')\n",
        "model = GPT2LMHeadModel(config)\n",
        "\n",
        "model_dir = '/content/kogpt2/pytorch_model.bin'\n",
        "\n",
        "# map_locationì„ í•´ì¤˜ì•¼ GPUì— ì˜¬ë¼ê°€ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n",
        "model.load_state_dict(torch.load(model_dir, map_location='cuda'), strict=False)\n",
        "model.to('cuda')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50000, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8Y7cgu9ohXP"
      },
      "source": [
        "### **Greedy Search**\n",
        "\n",
        "Greedy searchëŠ” ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì„ íƒí•  ë•Œ, ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ëŠ” ë‹¨ìˆœí•œ ë°©ë²•ì…ë‹ˆë‹¤.\n",
        "\n",
        "$w_t = argmax_{w}P(w | w_{1:t-1})$ at each timestep $t$. \n",
        "\n",
        "ë‹¤ìŒ ê·¸ë¦¼ì€ greedy searchë¥¼ ë‚˜íƒ€ë‚¸ ê²ƒ ì…ë‹ˆë‹¤.\n",
        "\n",
        "![Greedy Search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n",
        "\n",
        "\n",
        "ì•Œê³ ë¦¬ì¦˜ì€ $\\text{\"The\"}$ì—ì„œ ì‹œì‘í•˜ì—¬ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ $\\text{\"nice\"}$ ë“±ì„ ì„ íƒí•˜ëŠ” íƒìš•ìŠ¤ëŸ¬ìš´(Greedy) ë°©ë²•ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ìµœì¢…ì ìœ¼ë¡œ ìƒì„± ëœ word sequenceëŠ” $\\text{\"The\", \"nice\", \"woman\"}$ì´ê³  ì „ì²´ í™•ë¥ ì€ $0.5 \\times 0.4 = 0.2$ ì…ë‹ˆë‹¤.\n",
        "\n",
        "`Transformers` greedy searchë¥¼ ì‚¬ìš©í•´ë´…ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWLd_J6lXz_t",
        "outputId": "091d3f50-5df9-44bd-ab36-feffbb7c416b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "def tokenizing(text):\n",
        "    return torch.tensor(tokenizer.encode(text, add_special_tokens=False).ids).unsqueeze(0).to('cuda')\n",
        "\n",
        "input_ids = tokenizing(\"í¬í•­ì€\")\n",
        "\n",
        "# generate text until the output length (which includes the context length) reaches 100\n",
        "# ìƒì„± ëª¨ë¸ì€ generate í•¨ìˆ˜ë¥¼ í†µí•´ ë‹¤ìŒ tokenì„ ìƒì„±í•´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "# ê·¸ëƒ¥ ë„£ì–´ì£¼ë©´ ìë™ìœ¼ë¡œ greedy searchë¥¼ ì‹œì‘.\n",
        "greedy_output = model.generate(input_ids, max_length=100)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output.tolist()[0], skip_special_tokens=True))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "í¬í•­ì€ ì „ë°˜ 16ë¶„ ì¼€ë¹ˆì˜ ì„ ì œê³¨ë¡œ ì•ì„œê°”ì§€ë§Œ í›„ë°˜ 16ë¶„ ì¼€ë¹ˆì—ê²Œ ë™ì ê³¨ì„ í—ˆìš©í–ˆë‹¤.</s><s> ì´ ë•Œë¬¸ì— ì¼ê°ì—ì„œëŠ” 'ì•ˆì² ìˆ˜ íš¨ê³¼'ê°€ ëŒ€ì„  ì´í›„ ì´ì–´ì§ˆì§€, ì•„ë‹ˆë©´ 'ë°•ê·¼í˜œ ëŒ€ì„¸ë¡ 'ì´ ê¹¨ì§ˆì§€ ë“±ì„ ë‘ê³  'ì•ˆí’'ì´ ì¬ì—°ë  ìˆ˜ ìˆë‹¤ëŠ” ê´€ì¸¡ë„ ë‚˜ì˜¤ê³  ìˆë‹¤.</s><s> ì•ˆ í›„ë³´ëŠ” ì´ë‚  ì˜¤ì „ ê³µí‰ë™ ìº í”„ ì‚¬ë¬´ì‹¤ì—ì„œ ì—´ë¦° 'ìƒˆë¡œìš´ ì •ì¹˜, ìƒˆë¡œìš´ ë³€í™”ë¥¼ ìœ„í•œ ë¬¸ì¬ì¸ì˜ êµ¬ìƒ'ì´ë¼ëŠ” ì œëª©ì˜ ì •ì±…ë¹„ì „ ë°œí‘œíšŒì—ì„œ \"ëŒ€í†µë ¹ì´ ë˜ë©´ êµ­ë¯¼ê³¼ ì•½ì†í•œ ê²ƒì„ ë°˜ë“œì‹œ ì‹¤ì²œí•˜ê² ë‹¤\"ë©°\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT2ë¥¼ ì‚¬ìš©í•´ ì§§ì€ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ìƒì„±ëœ ë‹¨ì–´ë“¤ì˜ ë¬¸ë§¥ì€ í•©ë¦¬ì ì´ì§€ë§Œ, ëª¨ë¸ì´ ë°˜ë³µëœ ë‹¨ì–´ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤. \n",
        "\n",
        "ì´ëŸ¬í•œ í˜„ìƒì€ ì¼ë°˜ì ì¸ ì–¸ì–´ ìƒì„± ëª¨ë¸ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ê³µí†µëœ ë¬¸ì œì¸ë°, íŠ¹íˆ Greedy Searchì™€ Beam Searchì—ì„œ ê·¸ëŸ¬í•œ í˜„ìƒì´ ë”ìš± ë‘ë“œëŸ¬ì§€ê²Œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.\n",
        "\n",
        "Greedy Searchì˜ ì£¼ìš” ë‹¨ì ì€ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ë‚®ì€ í™•ë¥  ë‹¨ì–´ ì´í›„ì— ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ë” ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ë¥¼ ë†“ì¹œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. \n",
        "\n",
        "ì˜ˆë¥¼ ë“¤ì–´ ë‹¨ì–´ $\\text{has}$ëŠ” 0.9ë¡œ ë†’ì€ í™•ë¥ ì„ ê°–ì§€ë§Œ ì²«ë²ˆì§¸ ë‹¨ì–´ í›„ë³´ ì¤‘ ë‘ë²ˆì§¸ë¡œ ë†’ì€ conditional probabilityë¥¼ ê°–ëŠ” $\\text{dog}$ ì´í›„ì— ìˆ¨ì–´ ìˆëŠ” í˜•íƒœì…ë‹ˆë‹¤. ë”°ë¼ì„œ Greedy SearchëŠ” $\\text{\"The\"}, \\text{\"dog\"}, \\text{\"has\"}$ ë¼ëŠ” word sequenceë¥¼ ë†“ì¹˜ê²Œ ë©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "PN7h9WiFBJ2p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8DnXZ1WiuNd"
      },
      "source": [
        "### **Beam search**\n",
        "\n",
        "Beam searchëŠ” ê° time stepì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ hypothesisì˜ `num_beams`ë¥¼ ìœ ì§€í•˜ê³  ì „ì²´ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ hypothesisë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì¦‰, í•´ë‹¹ ì‹œì ì—ì„œ ìœ ë§í•œ ë¹”ì˜ ê°œìˆ˜ë§Œí¼(num_beams) ê³¨ë¼ì„œ ì§„í–‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë†’ì€ í™•ë¥ ì„ ê°€ì§€ê³  ìˆì§€ë§Œ ìˆ¨ì–´ìˆëŠ” word sequenceë¥¼ ë†“ì¹  ìœ„í—˜ì„ ì¤„ì…ë‹ˆë‹¤.ë‹¤ìŒ ê·¸ë¦¼ì€ `num_beams=2`ë¡œ ì„¤ì •í•œ Beam searchì˜ ì˜ˆì‹œì…ë‹ˆë‹¤:\n",
        "\n",
        "![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
        "\n",
        "\n",
        "Time step $1$ : ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ hypothesisì¸ $\\text{\"The\", \"nice\"}$ ì™¸ì—ë„ beam searchëŠ” ë‘ë²ˆì§¸ë¡œ ê°€ëŠ¥ì„±ì´ ë†’ì€ $\\text{\"The\", \"dog\"}$ë¥¼ ì¶”ì í•©ë‹ˆë‹¤. \n",
        "\n",
        "Time step $2$ : beam searchëŠ” $0.2$ì˜ ê°€ëŠ¥ì„±ì„ ê°€ì§„  $\\text{\"The\", \"nice\", \"woman\"}$ë³´ë‹¤ $0.36$ì˜ ê°€ëŠ¥ì„±ì„ ê°€ì§„ $\\text{\"The\", \"dog\", \"has\"}$ê°€ í™•ë¥ ì´ ë” ë†’ë‹¤ëŠ” ê²ƒì„ ì°¾ìŠµë‹ˆë‹¤. \n",
        "\n",
        "ì´ ë°©ë²•ì€ ìš°ë¦¬ì˜ toy exampleì—ì„œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ word sequenceë¥¼ ì°¾ì•„ëƒˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "Beam searchëŠ” í•­ìƒ Greedy searchë³´ë‹¤ ë†’ì€ í™•ë¥ ì˜ ê²°ê³¼ sequenceë¥¼ ì°¾ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ì´ê²Œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê²°ê³¼ë¼ê³ ëŠ” ë³´ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
        "\n",
        "`transformers`ì—ì„œ beam searchë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ ë´…ì‹œë‹¤. ìš°ë¦¬ëŠ” `num_beams > 1`, `early_stopping=True` ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  beam hypothesisê°€ eosí† í°ì— ë‹¿ìœ¼ë©´ ìƒì„±ì„ ë§ˆì¹˜ë„ë¡ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1R5kx30Ynej",
        "outputId": "1c2151e5-0db3-416c-a62c-fb287cb319c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# activate beam search and early_stopping\n",
        "beam_output = model.generate(\n",
        "    input_ids,  \n",
        "    max_length=100, \n",
        "    num_beams=5, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "í¬í•­ì€ 2ì¼ ì˜¤í›„ 7ì‹œ í¬í•­ ìŠ¤í‹¸ì•¼ë“œì—ì„œ í¬í•­ ìŠ¤í‹¸ëŸ¬ìŠ¤ì™€ Kë¦¬ê·¸ í´ë˜ì‹ 25ë¼ìš´ë“œ ì›ì •ê²½ê¸°ë¥¼ ê°–ëŠ”ë‹¤.</s><s> í•œí¸, ì´ë‚  ë°©ì†¡ì—ì„œëŠ” ì´ìˆ™ì˜(ì´ê²½ìˆ™ ë¶„)ê³¼ ê°•ì§€ë…•(ì´ë³´ì˜ ë¶„)ì˜ ëŸ¬ë¸ŒìŠ¤í† ë¦¬ê°€ ì „íŒŒë¥¼ íƒ”ë‹¤.</s><s> [ì‚¬ì§„]ë¥˜í˜„ì§„,'ë‹¤ì €ìŠ¤ íƒ€ì„ ì´ ë„ì™€ì¤˜'</s><s> ë‹¤ì €ìŠ¤ëŠ” 2ì¼(í•œêµ­ì‹œê°„) ë¯¸êµ­ ë¡œìŠ¤ì•¤ì ¤ë ˆìŠ¤ ë‹¤ì €ìŠ¤íƒ€ë””ì›€ì—ì„œ ì—´ë¦¬ëŠ” ì‹ ì‹œë‚´í‹° ë ˆì¦ˆì™€ì˜ ê²½ê¸°ì— ë¥˜í˜„ì§„ì„ ì„ ë°œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê²°ê³¼ëŠ” ë” ìœ ì°½í•˜ê²Œ ë³´ì´ì§€ë§Œ ì—¬ì „íˆ ë™ì¼í•œ word sequenceë¥¼ ë°˜ë³µí•˜ëŠ” ë¬¸ì œë¥¼ í¬í•¨í•©ë‹ˆë‹¤. \n",
        "\n",
        "ë‹¨ìˆœí•œ í•´ê²°ë²•ì€ [Paulus et al. (2017)](https://arxiv.org/abs/1705.04304)ì™€ [Klein et al. (2017)](https://arxiv.org/abs/1701.02810)ì˜ ë…¼ë¬¸ì—ì„œ ì œì•ˆ ëœ n-grams í˜ë„í‹°ë¥¼ ë„ì…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê°€ì¥ ì¼ë°˜ì ì¸ n-grams í˜ë„í‹°ëŠ” ì´ë¯¸ ë‚˜íƒ€ë‚œ n-gramì— ëŒ€í•´ ë‹¤ìŒ ë‹¨ì–´ë¡œ ìƒì„± ë  í™•ë¥ ì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë‘ë²ˆ ë‚˜íƒ€ë‚˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. \n",
        "\n",
        "`no_repeat_ngram_size=2`ë¥¼ ì„¤ì •í•´ 2-gramì´ ë‘ë²ˆ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì„ ë§‰ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "Yi0sRvZwGHos"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy3iVJgfnkMi",
        "outputId": "0871dfb7-a565-4622-8f7a-7a83cab69ed3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set no_repeat_ngram_size to 2\n",
        "beam_output = model.generate(\n",
        "    input_ids, \n",
        "    max_length=100, \n",
        "    num_beams=5, \n",
        "    no_repeat_ngram_size=2, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "í¬í•­ì€ ì§€ë‚œ 2ì¼ í¬í•­ ìŠ¤í‹¸ì•¼ë“œì—ì„œ ì—´ë¦° FAì»µ 32ê°• í¬í•­ìŠ¤í‹¸ëŸ¬ìŠ¤ì™€ ê²½ê¸°ì„œ 1-1 ë¬´ìŠ¹ë¶€ë¥¼ ê±°ë’€ë‹¤.</s><s> ì´ ê°™ì€ ì‚¬ì‹¤ì€ ì§€ë‚œ 3ì¼ ë°©ì†¡ëœ MBC â€˜í™©ê¸ˆì–´ì¥-ë¬´ë¦íŒë„ì‚¬â€™ì—ì„œ í•œí˜œì§„ì´ í•œí˜œì§„ê³¼ì˜ ì—´ì•  ì‚¬ì‹¤ì„ ê³µê°œí•œ í›„ ê¸‰ì†ë„ë¡œ í¼ì§€ê²Œ ëë‹¤â€ê³  ì „í–ˆë‹¤. tvN â€˜í™”ì„±ì¸ XíŒŒì¼â€™ì— ì¶œì—°í•œ í™”ì„±ì¸ì€ â€œí•œí˜œì§„ê³¼ ì¶•êµ¬ì„ ìˆ˜ ê¸°ì„±ìš© ì„ ìˆ˜ê°€ ì—´ì• ì„¤ì´ ìˆë‹¤â€ë©° â€œê¸°ì„±ìš©ê³¼ ì—´ì• ì„¤ì€ ì‚¬ì‹¤ë¬´ê·¼ì´ë‹¤â€\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxsksOGDpmA0"
      },
      "source": [
        "ë”ì´ìƒ ë°˜ë³µì´ ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "í•˜ì§€ë§Œ, n-gram í˜ë„í‹°ëŠ” ì‹ ì¤‘í•˜ê²Œ ì‚¬ìš©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, city New Yorkì— ëŒ€í•´ ìƒì„±ëœ ê¸°ì‚¬ëŠ” n-gramì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. 2-gramì„ ì‚¬ìš©í•˜ê²Œ ë  ê²½ìš° ì‹œì˜ ì´ë¦„ì´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í•œ ë²ˆë§Œ ë‚˜íƒ€ë‚˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "Beam searchì˜ ë˜ ë‹¤ë¥¸ ì¤‘ìš”í•œ íŠ¹ì§•ì€ ìƒì„±ëœ Top beamì„ ë¹„êµí•˜ì—¬ ëª©ì ì— ê°€ì¥ ì í•©í•œ Beamì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "`Transformers`ì—ì„œ ìš°ë¦¬ëŠ” `num_return_sequences`ë¥¼ top-nê°œì˜ ë†’ì€ scoringì„ ê°€ì§„ beamì„ returní•  ê²ƒì¸ì§€ ì„¤ì • í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ë‹¨, num_return_sequencesëŠ” num_beamsë³´ë‹¤ ê°™ê±°ë‚˜ ì‘ì•„ì•¼í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ClO3VphqGp6",
        "outputId": "8bda2fa7-3a7b-41d5-b55e-0ade889ce7a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set return_num_sequences > 1\n",
        "beam_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=50, \n",
        "    num_beams=5, \n",
        "    no_repeat_ngram_size=2, \n",
        "    num_return_sequences=5, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# now we have 3 output sequences\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output.tolist(), skip_special_tokens=True)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: í¬í•­ì€ ì§€ë‚œ 2ì¼ í¬í•­ìŠ¤í‹¸ì•¼ë“œì—ì„œ ì—´ë¦° í¬í•­ ìŠ¤í‹¸ëŸ¬ìŠ¤ì™€ì˜ 2012ë…„ K-ë¦¬ê·¸ 35ë¼ìš´ë“œì—ì„œ 1ëŒ€1ë¡œ ë¹„ê²¼ë‹¤.</s><s> í•œí¸, ì´ë‚  ë°©ì†¡ì—ì„œëŠ” ì˜¤ì˜ì‹¤(ì†¡ì˜¥ìˆ™ ë¶„)ì´ ì˜¤ìˆ˜(ì¡°ì¸ì„±\n",
            "1: í¬í•­ì€ ì§€ë‚œ 2ì¼ í¬í•­ìŠ¤í‹¸ì•¼ë“œì—ì„œ ì—´ë¦° í¬í•­ ìŠ¤í‹¸ëŸ¬ìŠ¤ì™€ì˜ 2012ë…„ K-ë¦¬ê·¸ 35ë¼ìš´ë“œì—ì„œ 1ëŒ€1ë¡œ ë¹„ê²¼ë‹¤.</s><s> í•œí¸, ì´ë‚  ë°©ì†¡ì—ì„œëŠ” ì˜¤ì˜ì‹¤(ì†¡ì˜¥ìˆ™ ë¶„)ê³¼ ì˜¤ìˆ˜ì„±(ì˜¤\n",
            "2: í¬í•­ì€ ì§€ë‚œ 2ì¼ í¬í•­ìŠ¤í‹¸ì•¼ë“œì—ì„œ ì—´ë¦° í¬í•­ ìŠ¤í‹¸ëŸ¬ìŠ¤ì™€ì˜ 2012ë…„ K-ë¦¬ê·¸ 35ë¼ìš´ë“œì—ì„œ 1ëŒ€1ë¡œ ë¹„ê²¼ë‹¤.</s><s> í•œí¸, ì´ë‚  ë°©ì†¡ì—ì„œëŠ” ì˜¤ì˜ì‹¤(ì†¡ì˜¥ìˆ™ ë¶„)ì´ ì˜¤ìˆ˜ì„±(ì˜¤\n",
            "3: í¬í•­ì€ ì§€ë‚œ 2ì¼ í¬í•­ìŠ¤í‹¸ì•¼ë“œì—ì„œ ì—´ë¦° í¬í•­ ìŠ¤í‹¸ëŸ¬ìŠ¤ì™€ì˜ 2012ë…„ K-ë¦¬ê·¸ 35ë¼ìš´ë“œì—ì„œ 1ëŒ€1ë¡œ ë¹„ê²¼ë‹¤.</s><s> í•œí¸, ì´ë‚  ë°©ì†¡ì—ì„œëŠ” ì˜¤ì˜ì‹¤(ì†¡ì˜¥ìˆ™ ë¶„)ì´ ì˜¤ìˆ˜ì„±(ì„±\n",
            "4: í¬í•­ì€ ì§€ë‚œ 2ì¼ í¬í•­ìŠ¤í‹¸ì•¼ë“œì—ì„œ ì—´ë¦° í¬í•­ ìŠ¤í‹¸ëŸ¬ìŠ¤ì™€ì˜ 2012ë…„ K-ë¦¬ê·¸ 35ë¼ìš´ë“œì—ì„œ 1ëŒ€1ë¡œ ë¹„ê²¼ë‹¤.</s><s> í•œí¸, ì´ë‚  ë°©ì†¡ì—ì„œëŠ” ì˜¤ì˜ì‹¤(ì†¡ì˜¥ìˆ™ ë¶„)ì´ ì˜í¬(ì˜\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhLKyfdbsjXc"
      },
      "source": [
        "ìœ„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ Top 5ì˜ Beam hypothesisëŠ” ì„œë¡œ ì•½ê°„ë§Œ ë‹¤ë¥¼ ë¿ì´ë©° 5ê°œë§Œ ì‚¬ìš©í–ˆì„ ê²½ìš° ë³„ë¡œ ë†€ë„ë§Œí•œ ê²°ê³¼ëŠ” ì•„ë‹™ë‹ˆë‹¤.\n",
        "\n",
        "Open-ended generationì—ì„œ, beam searchê°€ ìµœì„ ì˜ ì„ íƒì´ ì•„ë‹ ìˆ˜ ìˆëŠ” ëª‡ê°€ì§€ ì´ìœ ê°€ ì œì‹œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "- Beam searchëŠ” Machine translation ë˜ëŠ” Text summarizationì²˜ëŸ¼ ì›í•˜ëŠ” ë¬¸ì¥ ìƒì„± ê¸¸ì´ê°€ ì˜ˆì¸¡ ê°€ëŠ¥í•œ taskì—ì„œëŠ” ì˜ ì‘ë™ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ Dialog ë˜ëŠ” Story generation taskì²˜ëŸ¼ ì¶œë ¥ ê¸¸ì´ê°€ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆëŠ” open-ended generationì—ì„œëŠ” ì›í™œí•˜ê²Œ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ - see [Murray et al. (2018)](https://arxiv.org/abs/1808.10006) and [Yang et al. (2018)](https://arxiv.org/abs/1808.09582). \n",
        "\n",
        "- Beam searchëŠ” ë°˜ë³µ ìƒì„± ë¬¸ì œì— ì·¨ì•½í•©ë‹ˆë‹¤. íŠ¹íˆ Story generation taskì—ì„œ n-gram ë˜ëŠ” ê¸°íƒ€ í˜ë„í‹°ë¥¼ í†µí•´ ë¬¸ì¥ì„ ì œì–´í•˜ëŠ” ê²ƒì´ ì–´ë µìŠµë‹ˆë‹¤. *ë°˜ë³µì´ ì—†ëŠ” êµ¬ë¬¸* ê³¼ *n-gram*ì˜ ë°˜ë³µ ì£¼ê¸° ì‚¬ì´ì—ì„œ ì ë‹¹í•œ trade-offë¥¼ ì°¾ê¸° ìœ„í•´ ë§ì€ fine-tuningì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "- [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751)ëŠ” ê³ í’ˆì§ˆ ì¸ê°„ ì–¸ì–´ëŠ” ë†’ì€ í™•ë¥ ì˜ ë‹¤ìŒ ë‹¨ì–´ ë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤. ì‰½ê²Œ ë§í•˜ìë©´ ì¸ê°„ ì…ì¥ì—ì„œ ìš°ë¦¬ëŠ” ì§€ë£¨í•˜ê±°ë‚˜ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë¬¸ì¥ì´ ì•„ë‹ˆë¼ ìš°ë¦¬ë¥¼ ë†€ë¼ê²Œ í•  ìˆ˜ ìˆëŠ” ë¬¸ì¥ ìƒì„±ì„ ì›í•œë‹¤ê³  í•©ë‹ˆë‹¤. ì €ìëŠ” ëª¨ë¸ì´ human text vs. beam seachë¥¼ graphë¡œ ë³´ì—¬ì£¼ë©´ì„œ beam search textê°€ ê·¸ë‹¤ì§€ ë†€ëì§€ ì•Šì€ ë¬¸ì¥ì„ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)\n",
        "\n",
        "\n",
        "So let's stop being boring and introduce some randomness ğŸ¤ª."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbbIyK84wHq6"
      },
      "source": [
        "### **Sampling**\n",
        "\n",
        "ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœì˜ samplingì€ ì¡°ê±´ë¶€ í™•ë¥  ë¶„í¬ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ì–´ $w_t$ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "$$w_t \\sim P(w|w_{1:t-1})$$\n",
        "\n",
        "ì•„ë˜ ì‚¬ì§„ì€ sampling í•  ë•Œ, ì–¸ì–´ ìƒì„±ì„ ì‹œê°í™”í•œ í˜•íƒœì…ë‹ˆë‹¤.\n",
        "\n",
        "![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n",
        "\n",
        "Samplingì„ ì´ìš©í•œ ì–¸ì–´ ìƒì„±ì€ ë”ì´ìƒ *deterministic*í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. \n",
        "\n",
        "ë‹¨ì–´ $\\text{\"car\"}$ ëŠ” contional probability distribution $P(w | \\text{\"The\"})$ì—ì„œ ìƒ˜í”Œë§ ë˜ê³ , $P(w | \\text{\"The\"}, \\text{\"car\"})$ëŠ” $\\text{\"drives\"}$ë¥¼ ìƒ˜í”Œë§ í•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "`Transformers`ì—ì„œ ìš°ë¦¬ëŠ” `do_sample=True` ë¥¼ ì„¤ì •í•˜ê³  `top_k=0`ë¡œ ë‘ì–´ *Top-K* ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.(ë’¤ì—ì„œ ë‹¤ë£° ê²ƒ)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "519f09d4-4a45-42e0-f825-e71a25927d25",
        "id": "aRAz4D-Ks0_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# torch.manual_seed(53) #ì›í•œë‹¤ë©´ random seedë¥¼ ì§€ì • í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "í¬í•­ì€ ë°˜ë©´, ëŒ€ì•ˆë™ì€ ì„¤ìš• ì˜ì§€ë¥¼ ì•ì„¸ì›Œ ìƒìŠ¹ì„¸ë‹¤.</s><s> ì•„ì´ë¦°ì´ ìƒí›„ 7ê°œì›” ëœ ë‚˜ì—ê²ŒëŠ” ì§€ê¸ˆì´ë¼ë„ ì¶œê°„ëœ ì•„ë™ë„ì„œì„ ë¬¼ì„ ê³ë“¤ì´ëŠ” ì¬ëŠ¥ê¸°ë¶€ë¡œ ë³´ë‹µí–ˆë‹¤.</s><s> ê³ ì†Œì§„ì€ ë˜ íë§ìº í”„ì—ì„œ ë§¤ì¼ ê°™ì´ ë¼ë©´ì„œ ê·¸ë¦¼ì±… 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê´œì°®ì•„ ë³´ì´ì§€ë§Œ ì¼ê´€ì„±ì´ ì—†ìŠµë‹ˆë‹¤. ì´ê²ƒì€ sampling word sequencesë¥¼ í• ë•Œ ëª¨ë¸ì´ ì¼ê´€ì„±ì—†ì´ íš¡ì„¤ìˆ˜ì„¤í•˜ëŠ” ë¬¸ì¥ì„ ë°œìƒì‹œí‚¤ëŠ” í° ë¬¸ì œì…ë‹ˆë‹¤. ([Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751)).\n",
        "ëª¨ë¸ì´ ë§Œë“¤ì–´ ë‚¸ í™•ë¥ ì´ smoothí•œ ë‚˜ë¨¸ì§€, ë‚®ì€ í™•ë¥ ì˜ í† í°ì´ ì§€ë‚˜ì¹˜ê²Œ ì˜ ìƒ˜í”Œë§ ë˜ëŠ” ê²ƒì´ ì›ì¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "í•œê°€ì§€ íŠ¸ë¦­ì€ [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max) ì˜ ì´ë¥¸ë°” `temperature`ë¥¼ ë‚®ì¶”ì–´ ë¶„í¬ $P(w|w_{1:t-1})$ë¥¼ ë” ì„ ëª…í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ì˜ ê°€ëŠ¥ì„±ì€ ì¦ê°€ì‹œí‚¤ê³  ë‚®ì€ í™•ë¥ ì˜ ë‹¨ì–´ ê°€ëŠ¥ì„±ì€ ê°ì†Œì‹œí‚¤ëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. \n",
        "temperatureê°€ 0ì— ê°€ê¹Œì›Œì§ˆ ìˆ˜ë¡ greedy decodingì— ê°€ê¹Œìš´ ì•„ì›ƒí’‹ì´ ë‚˜ì˜µë‹ˆë‹¤.\n",
        "\n",
        "temperatureë¥¼ ì ìš©í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê·¸ë¦¼ì„ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/sampling_search_with_temp.png?raw=true)\n",
        "\n",
        "step=1ì˜ ë‹¤ìŒ ë‹¨ì–´ ë¶„í¬ëŠ” ë”ìš± ì„ ëª…í•´ì¡Œê¸° ë•Œë¬¸ì— ë‹¨ì–´ $\\text{\"car\"}$ë¥¼ ì„ íƒí•  í™•ë¥ ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "`temperature=0.7`ë¥¼ ì„¤ì •í•˜ì—¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë¶„í¬ë¥¼ ì–´ë–»ê²Œ ë³€í™”ì‹œí‚¤ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "croVNefgNC9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=100, \n",
        "    top_k=0, \n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIBRDVLSOmpC",
        "outputId": "a9e39520-d22c-48f2-d599-a9fd5b89d99b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "í¬í•­ì€ ëŒ€êµ¬FCì™€ì˜ í™ˆê²½ê¸°ì—ì„œ 1-1 ë¬´ìŠ¹ë¶€ë¥¼ ê±°ë’€ë‹¤.</s><s> ë˜í•œ, ëŒ€êµ¬ONE FCëŠ” ì˜¤ëŠ” 26ì¼ ì¸ì²œ ìœ ë‚˜ì´í‹°ë“œë¡œ FC ì„œìš¸ê³¼ì˜ í™ˆê²½ê¸°ë¥¼ ì•ë‘ê³  ìˆë‹¤.</s><s> ê²½ê¸° ì „ í•œí˜œì§„ì€ \"ì•½ì†í•œ ëŒ€ë¡œ ìƒµì´ë“ , ì¹´í˜ë“  ì•ˆ ê°€ë©´ ë˜ì§€ ì•Šê³  ê·¸ëƒ¥ ë§Œë‚˜ì\"ë©° ë©”ì„¸ì§€ë¥¼ ì „í–ˆë‹¤.</s><s> í•œí¸, í•œí˜œì§„ì€ ì˜¤ëŠ” 7ì›” 1ì¼ ê¸°ì„±ìš©ê³¼ ì„œìš¸ ê°•ë‚¨êµ¬ ì‚¼ì„±ë™ ì¸í„°ì½˜í‹°ë„¨íƒˆ í˜¸í…”ì—ì„œ í˜¼ì „ì„ì‹ ê³¼ ì¶•êµ¬ì„ ìˆ˜ ê¸°ì„±ìš©ê³¼ì˜\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ì œ ì´ìƒí•œ n-gramì´ ì ê³  ì¶œë ¥ ë¬¸ì¥ì´ ì¡°ê¸ˆ ë” ì¼ê´€ì„± ìˆê²Œ ìƒì„±ë©ë‹ˆë‹¤. temperatureë¥¼ ì ìš©í•˜ë©´ ë¶„í¬ê°€ ëœ randomí•˜ì§€ë§Œ `temperature` $ \\to 0$ë¡œ ì„¤ì •í•œë‹¤ë©´ temperatureê°€ ì ìš©ëœ samplingì€ greedy decodingê³¼ ê°™ì•„ì§€ë©° ì´ì „ê³¼ ë™ì¼í•œ ë¬¸ì œë¥¼ ê²ªê²Œ ë©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "TsjsAeI3Ot2w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binNTroyzQBu"
      },
      "source": [
        "### **Top-K Sampling**\n",
        "\n",
        "[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf) ì€ ê°„ë‹¨í•˜ì§€ë§Œ ë§¤ìš° ê°•ë ¥í•œ ìƒ˜í”Œë§ ë°©ì‹ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. . *Top-K* samplingì—ì„œ ë†’ì€ ê°€ëŠ¥ì„±ì„ ê°€ì§„ kê°œë¥¼ ì œì™¸í•œ ë‹¨ì–´ëŠ” í•„í„°ë§ ë˜ê³  k ì´í›„ì˜ probablity massëŠ” ì¬ë¶„ë°°ë©ë‹ˆë‹¤. GPT2ëŠ” Top-K Samplingë°©ì‹ì„ ì±„íƒí–ˆëŠ”ë°, ì´ê²ƒì´ Story Gerneration Taskì— ì„±ê³µí•œ ì´ìœ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n",
        "\n",
        "Top-K Samplingì„ ë” ì˜ ì„¤ëª…í•˜ê¸° ìœ„í•´ ìœ„ì˜ ì˜ˆì œì—ì„œ ë‘ Sampling stepì— ì‚¬ìš©ë˜ëŠ” ë²”ìœ„ë¥¼ 3ë‹¨ì–´ì—ì„œ 10ë‹¨ì–´ë¡œ í™•ì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n",
        "\n",
        "\n",
        "K=6ì„ ì„¤ì •í•˜ë©´ ë‘ Sampling stepsì—ì„œ Sampling poolì„ 6ê°œì˜ ë‹¨ì–´ë¡œ ì œí•œí•©ë‹ˆë‹¤.\n",
        "\n",
        "$\\text{\"The\"}$ ë‹¤ìŒìœ¼ë¡œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” 6ê°œë¥¼ ì„ íƒí•˜ê³  $\\text{\"The:, \"car\"}$ ë’¤ì— ì˜¬ ìˆ˜ ìˆëŠ” 6ê°œë¥¼ ì„ íƒí•©ë‹ˆë‹¤. \n",
        "\n",
        "ì²« stepì—ì„œ ì „ì²´ í™•ë¥  ì§ˆëŸ‰ì˜ 2/3ì¸ 0.68ì •ë„ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ì—ì„œ ë””ì½”ë”©ë˜ì§€ë§Œ, ë‘ë²ˆì§¸ stepì—ì„œ ê±°ì˜ ëª¨ë“  í™•ë¥ ì§ˆëŸ‰ì¸ 0.99ì—ì„œ ë””ì½”ë”©í•©ë‹ˆë‹¤.\n",
        "\n",
        "ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ê·¸ê²ƒì´ ë‘ë²ˆì§¸ sampling stepì—ì„œ $\\text{\"not\", \"the\", \"small\", \"told\"}$ ì™€ ê°™ì€ ë‹¤ì†Œ ì´ìƒí•œ í›„ë³´ë“¤ì„ ì„±ê³µì ìœ¼ë¡œ ì œê±°ê°€ ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "200tV_DPQCvn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBtDOdD0wx3l",
        "outputId": "84ac62e5-9611-4948-f517-d57dfea49828",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set top_k to 50\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "í¬í•­ì€ ìš¸ì‚°ê³¼ 0-0 ë¬´ìŠ¹ë¶€, ëŒ€êµ¬ëŠ” ì „ë¶ì˜ í™ˆ ì•¤ë“œ ì–´ì›¨ì´ 4ê°•ì— ì§„ì¶œí–ˆë‹¤.</s><s> ì´ì–´ \"ì´ë“¤ì´ ì£¼ì¥í•˜ëŠ” ê²ƒì´ ë°”ë¡œ ë°”ë¡œ í•œ ë²ˆë„ ê²½í—˜í•˜ì§€ ëª»í•œ ìƒˆë¡œìš´ ë°©ì‹ìœ¼ë¡œ ìš°ë¦¬ì˜ ìƒê°ì„ í‘œí˜„í•˜ê³  ìš°ë¦¬ êµ­ë¯¼ì—ê²Œ ì¹´íƒ€ë¥´ì‹œìŠ¤ë¥¼ ë“œë¦¬ë©° ìƒˆë¡œìš´ ë¯¸ë˜ë¥¼ ë§Œë“œëŠ”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y77H5m4ZmhEX"
      },
      "source": [
        "ì§€ê¸ˆê¹Œì§€ ë´ì˜¨ decoding methods ì¤‘ ê°€ì¥ ì‚¬ëŒë‹¤ì›Œ ë³´ì´ëŠ” í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. Top-K Samplingì˜ í•œê°€ì§€ ìš°ë ¤ë˜ëŠ” ì ì€ ë‹¤ìŒ ë‹¨ì–´ í™•ë¥  ë¶„í¬ $P(w|w_{1:t-1})$ì—ì„œ í•„í„°ë§ ëœ ë‹¨ì–´ ìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ì§€ ì•ŠëŠ” ì ì…ë‹ˆë‹¤. ì˜ˆë¥¼ë“¤ì–´ ìœ„ ê·¸ë¦¼ì—ì„œ ì²«ë²ˆì§¸ stepì˜ ë‹¨ì–´ë“¤ì€ ì „ë°˜ì ìœ¼ë¡œ í‰í‰í•œ ë¶„í¬ì—ì„œ samplingë˜ì§€ë§Œ, ë‘ë²ˆì§¸ stepì˜ ì–´ë–¤ ë‹¨ì–´ë“¤ì€ ë§¤ìš° sharpí•œ ë¶„í¬ì—ì„œ sampling ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "Step $t=1$ì—ì„œ Top-Kì€ ê½¤ í•©ë¦¬ì ì¸ í›„ë³´ì²˜ëŸ¼ ë³´ì´ëŠ” $\\text{\"people\", \"big\", \"house\", \"cat\"}$ì„ ìƒ˜í”Œë§í•˜ëŠ” ê°€ëŠ¥ì„±ì„ ë°°ì œí•©ë‹ˆë‹¤. ë°˜ë©´ì— Step $t=2$ì—ì„œ ë‹¨ì–´ Sample poolì— ë‹¨ì–´ $\\text{\"down\", \"a\"}$ì™€ ê°™ì€ ë¶€ì ì ˆí•œ ë‹¨ì–´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ Sample poolì´ ê³ ì •í¬ê¸° Kë¡œ ì œí•œë˜ë©´ ëª¨í˜•ì´ Sharpí•œ ë¶„í¬ì—ì„œ íš¡ì„¤ìˆ˜ì„¤í•œ ë‹¨ì–´ë¥¼ ê³ ë¥¼ ìœ„í—˜ì´ìˆê³  í‰í‰í•œ ë¶„í¬ì—ì„œëŠ” ë¬¸ì¥ì˜ ì°½ì˜ì„±ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ([Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki9LAaexzV3H"
      },
      "source": [
        "### **Top-p (nucleus) sampling**\n",
        "\n",
        "*Top-p* samplingì€ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ Kê°œì—ì„œë§Œ sampleì„ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ëˆ„ì  í™•ë¥ ì´ í™•ë¥  pë¥¼ ì´ˆê³¼í•˜ëŠ” ìµœì†Œí•œì˜ ë‹¨ì–´ ì§‘í•©ì—ì„œ sampleì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "\n",
        "ê·¸ í›„ í™•ë¥  ì§ˆëŸ‰ì´ ë‹¨ì–´ ì§‘í•© ì‚¬ì´ì— ì¬ë¶„ë°° ë©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë‹¤ìŒ ë‹¨ì–´ì˜ í™•ë¥  ë¶„í¬ì— ë”°ë¼ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ê°€ ë™ì ìœ¼ë¡œ ì¦ê°€í•˜ê±°ë‚˜ ê°ì†Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n",
        "\n",
        "$p=0.92$ë¡œ ì„¤ì •í•  ê²½ìš°, *Top-p* ëŠ” $p=92\\%$ë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ë‹¨ì–´ ìˆ˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ì²«ë²ˆì§¸ ìŠ¤í…ì—ì„œ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ë‹¨ì–´ 9ê°œê°€ í¬í•¨ëœ ë°˜ë©´, ë‘ë²ˆì§¸ ìŠ¤í…ì—ì„œëŠ” top 3ê°œë§Œ ì„ íƒí•´ë„ $p=92\\%$ë¥¼ ì´ˆê³¼í•˜ê²Œ ë©ë‹ˆë‹¤. ì¦‰, ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ì—ë§Œ samplingì„ í•˜ê³  ê·¸ë ‡ì§€ ì•Šì€ ë‹¨ì–´ëŠ” samplingí•  í™•ë¥ ì´ ë§¤ìš° ì ì–´ì§‘ë‹ˆë‹¤.\n",
        "\n",
        "`Transformers`ì—ì„œ `top_p âˆˆ (0,1)`ì„ ì„¤ì •í•˜ì—¬ *Top-p* samplingì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvwIc7YAx77F",
        "outputId": "a4c330d8-9bd6-4eb1-ea8a-1a1337406352",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# deactivate top_k sampling and sample only from 92% most likely words\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_p=0.92, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "í¬í•­ì€ ê³¨ì„ ë„£ìœ¼ë©´ 4ë§Œ ë‹¬ëŸ¬ì˜ ìƒê¸ˆì„ ê±°ë‹ˆ ì¸ìˆ˜ê¶Œì„ ë”°ë‚¸ë‹¤ëŠ” ì›ì¹™ì„ ì„¸ì› ë‹¤.</s><s> ì´ì™€ ê´€ë ¨í•´ ì „ë¬¸ê°€ë“¤ì€ ì§€ë‚œ 2005ë…„ ê°œì •ëœ êµ­ë‚´í•­ê³µëª¨í•¨ë²•ì˜ ìœ„í—Œì„± ì—¬ë¶€ íŒê²° ë‹¹ì‹œ í—Œì¬ì˜ ì…ì¥ì€ í•©í—Œì¼ ê°€ëŠ¥ì„±ì´ í¬ë‹¤ê³  ë³´ê³  ìˆë‹¤.</s><s> ì–´ëŠ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn-8gLaR4lat"
      },
      "source": [
        "ì´ë¡ ì ìœ¼ë¡œëŠ” *Top-p*ê°€ *Top-K*ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì•„ ë³´ì´ì§€ë§Œ, ë‘ ë°©ë²• ëª¨ë‘ ì‹¤ì œë¡œ ì˜ ì‘ë™í•©ë‹ˆë‹¤. \n",
        "\n",
        "*Top-p*ì™€ *Top-K*ëŠ” í•¨ê»˜ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” ë§¤ìš° ë‚®ì€ ìˆœìœ„ì˜ ë‹¨ì–´ë¥¼ í”¼í•˜ë©´ì„œë„ ì¼ë¶€ ë™ì  ì„ íƒì„ í—ˆìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ë…ë¦½ì ìœ¼ë¡œ samplingëœ multiple outputsë¥¼ ì–»ê¸° ìœ„í•´ `num_return_sequences > 1`ë¡œ ì„¤ì •í•´ë´…ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kY8P9VG8Gi9",
        "outputId": "8f2368e6-a17f-4c95-c7b4-77cdb30ad857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=50, \n",
        "    top_p=0.95, \n",
        "    num_return_sequences=5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output.tolist(), skip_special_tokens=True)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: í¬í•­ì€ 25ì¼ í™ˆ êµ¬ì¥ì¸ í¬í•­ ìŠ¤í‹¸ëŸ¬ìŠ¤ì„œ ì—´ë¦° í¬í•­ ìŠ¤í‹¸ëŸ¬ìŠ¤ì™€ 4ê°• 2ì°¨ì „ í™ˆê²½ê¸°ì— ì¼€ë¹ˆê³¼ ê¹€í˜•ì¤€ì„ ì˜ì…í•˜ë©° ê³µê²©ì§„ì˜ íŒŒê´´ë ¥ì„ ì˜ˆê³ í–ˆë‹¤.</s><s> ë˜ ê°™ì€ ë‚  ì—´ë¦° 1ì°¨ì „ì—ì„œëŠ” Kë¦¬ê·¸ í´ë˜ì‹ 3ì—°\n",
            "1: í¬í•­ì€ ìµœê·¼ Kë¦¬ê·¸ í´ë˜ì‹ 2ê²½ê¸°ì—ì„œ 2ìŠ¹ 2ë¬´ë¥¼ ê±°ë’€ë‹¤.</s><s> \"ê²½ì°° ìˆ˜ì‚¬ìë£Œ, ë…¹ì·¨ë¡ ë¶„ì„ê²°ê³¼ ë°œí‘œí•˜ê² ë‹¤\"</s><s> \"ê²½ì°° ìˆ˜ì‚¬ìë£Œ, ë…¹ì·¨ë¡ ë¶„ì„ê²°ê³¼ ë°œí‘œí•˜ê² ë‹¤\"</s><s> 'ë‚´ë€ìŒëª¨' í˜ì˜ë¡œ ìˆ˜ì‚¬ë¥¼ ë°›ê³ \n",
            "2: í¬í•­ì€ 2012ì‹œì¦Œë¶€í„° â€˜í˜„ëŒ€ì˜¤ì¼ë±…í¬ Kë¦¬ê·¸â€™ì˜ ìŠ¤í°ì„œë¥¼ ë§¡ìœ¼ë©° ìš¸ì‚°ì€ ë”ìš± ê°•í•´ì§„ ëª¨ìŠµì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤.</s><s> í•˜ì§€ë§Œ 'ì‹ ì‚¬ì˜ í’ˆê²©'ì˜ ìœ¤ì„¸ì•„, ê¹€ì§€ì„, ê¹€ì¬ì›, ì´ì¢…í˜, ìœ¤ìƒí˜„ ë“±ì´ 'ë‹¤\n",
            "3: í¬í•­ì€ 3ì¼ ì˜¤í›„ 6ì‹œ í„°í‚¤ ì•™ì¹´ë¼ êµ¬ì¥ì—ì„œ ì•Œ ì´í‹°ë¬´ì™€ ì¹œì„ ê²½ê¸°ë¥¼ ì¹˜ë¥¼ ì˜ˆì •ì´ì—ˆì§€ë§Œ ìš¸ì‚°ìœ¼ë¡œ ì—°ê³ ì§€ê°€ ë³€ê²½ëë‹¤.</s><s> ê·¸ëŸ¬ë‚˜ ì´ ì˜í™”ëŠ” ì• ì´ˆ 2008ë…„ 7ì›”ì— ê°œë´‰ë  ì˜ˆì •ì´ì—ˆìœ¼ë‚˜, ê·¸ì „ì— ë‹¤ë¥¸ ë°°ìš°ë“¤ì´ ìºìŠ¤íŒ… ë¬¸ì œë¡œ ê°ˆë“±ì„ ë¹š\n",
            "4: í¬í•­ì€ ì´ê²¼ì§€ë§Œ 2ê²½ê¸°ì—ì„œëŠ” ê³¨ë“ì‹¤ì—ì„œ ë°€ë ¤ 2ìœ„ë¥¼ ë‹¬ë¦¬ëŠ” ë“± ìµœê·¼ 5ë…„ ë™ì•ˆ ì •ê·œë¦¬ê·¸ì—ì„œ ë‹¨ í•œ ë²ˆë„ 4ê°•ì— ë“¤ì§€ ëª»í•˜ëŠ” ë¶€ì§„ì„ ê²ªê³  ìˆë‹¤.</s><s> â€˜ë‚´ ë”¸ ì„œì˜ì´â€™ì˜ ì´ìƒìœ¤ ì—­ì‹œ ìì‹ ì˜ ë§ˆìŒì„ ìˆ¨ê¸¸ ìˆ˜\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vRPfMl88rk0"
      },
      "source": [
        "Cool, now you should have all the tools to let your model write your stories with `transformers`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsWd7e98Vcs3"
      },
      "source": [
        "### **Conclusion**\n",
        "\n",
        "*ad-hoc* decoding methodsì— ë”°ë¥´ë©´ open-ended generationì—ì„œ *top-p* and *top-K*ëŠ” *greedy*, *beam search*ë³´ë‹¤ ë”ìš± ìœ ì°½í•œ textë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "í•˜ì§€ë§Œ  [Welleck et al. (2020)](https://arxiv.org/abs/2002.02492)ì— ë”°ë¥´ë©´ *top-K*ì™€ *top-p*ëŠ” ì—¬ì „íˆ ë°˜ë³µë˜ëŠ” word sequencesë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œ(*greedy*, *beam search*ì™€ ê°™ì´)ë¥¼ ê²ªê³  ìˆë‹¤ê³  í•©ë‹ˆë‹¤.\n",
        "\n",
        "[Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf)ëŠ” human evalutation ê´€ì ì—ì„œ, model training ëª©ì  í•¨ìˆ˜ë¥¼ ì˜ ì¡°ì •í•˜ë©´, *beam search*ê°€ *Top-p*ë³´ë‹¤ ìœ ì°½í•œ textë¥¼ ìƒì„±í•œë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "Open-ended language generationì€ ë¹ ë¥´ê²Œ ë°œì „í•˜ëŠ” ë¶„ì•¼ì´ë©°, ë¬´ì—‡ì´ ì í•©í•˜ë‹¤ê³  ë‹¨ì •í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ì—ì„œ ê°€ì¥ ì˜ ì‘ë™í•˜ëŠ” ë°©ë²•ì´ ë¬´ì—‡ì¸ì§€ ê³ ë ¤í•´ì•¼í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4CYi91h11yd"
      },
      "source": [
        "### **Appendix**\n",
        "\n",
        "There are a couple of additional parameters for the `generate` method that were not mentioned above. We will explain them here briefly!\n",
        "\n",
        "- `min_length` can be used to force the model to not produce an EOS token (= not finish the sentence) before `min_length` is reached. This is used quite frequently in summarization, but can be useful in general if the user wants to have longer outputs.\n",
        "- `repetition_penalty` can be used to penalize words that were already generated or belong to the context. It was first introduced by [Kesker et al. (2019)](https://arxiv.org/abs/1909.05858) and is also used in the training objective in [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf). It can be quite effective at preventing repetitions, but seems to be very sensitive to different models and use cases, *e.g.* see this [discussion](https://github.com/huggingface/transformers/pull/2303) on Github.\n",
        "\n",
        "- `attention_mask` can be used to mask padded tokens\n",
        "- `pad_token_id`, `bos_token_id`, `eos_token_id`: If the model does not have those tokens by default, the user can manually choose other token ids to represent them.\n",
        "\n",
        "For more information please also look into the `generate` function [docstring](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate)."
      ]
    }
  ]
}